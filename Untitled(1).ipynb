{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p-b1QDFdvrTg",
        "outputId": "3c293239-118a-408c-cd22-592bc6240f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "zEPAopN7v_Ii"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "lADQ6kQe8cvk"
      },
      "outputs": [],
      "source": [
        "def predict_next_7_days(model, scaler, last_known_data):\n",
        "    predictions = []\n",
        "    input_data = last_known_data.reshape(1, 1, -1)\n",
        "    for _ in range(7):\n",
        "        next_pred = model.predict(input_data, verbose=0)\n",
        "        predictions.append(next_pred[0, 0])\n",
        "\n",
        "        # Update input_data with the new prediction\n",
        "        next_input = np.roll(input_data, -1, axis=2)\n",
        "        next_input[0, 0, -1] = next_pred\n",
        "        input_data = next_input\n",
        "\n",
        "    # Create a placeholder with the correct shape for inverse_transform\n",
        "    predictions_full = np.zeros((len(predictions), scaled_data.shape[1]))\n",
        "\n",
        "    # Assign predictions to the first column (representing 'Close')\n",
        "    predictions_full[:, 0] = predictions\n",
        "\n",
        "    # Inverse transform and return the 'Close' column\n",
        "    return scaler.inverse_transform(predictions_full)[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "01AWvswlA0lM"
      },
      "outputs": [],
      "source": [
        "def predict_for_user_input(ticker_value, date_input):\n",
        "    # Load data for the specific ticker\n",
        "    data = pd.read_csv(f'TRAIN/{ticker_value}.csv')\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Rolling_Avg'] = data['Close'].rolling(window=window_size).mean()\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    for i in range(1, 16):\n",
        "      data[f'Lag_{i}'] = data['Close'].shift(i)\n",
        "\n",
        "  # Drop rows with NaN values caused by lagging\n",
        "  #data.dropna(inplace=True) # Original line causing the error\n",
        "  # Instead of dropping all rows with NaN, drop only those where all Lag columns are NaN\n",
        "    data.dropna(subset=[f'Lag_{i}' for i in range(1, 11)], how='all', inplace=True)\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data[['Close'] + [f'Lag_{i}' for i in range(1, 11)]])\n",
        "\n",
        "\n",
        "    # Initialize predicted_values to an empty list to avoid UnboundLocalError\n",
        "    predicted_values = []  # This line is added\n",
        "\n",
        "    if date_input in data['Date'].values:\n",
        "        print(f\"Date {date_input} exists in the dataset.\")\n",
        "        # ... (rest of your code remains the same) ...\n",
        "\n",
        "    else:\n",
        "        print(f\"Date {date_input} is not present in the dataset. Predicting future values...\")\n",
        "        #  Get the last 15 values, including Rolling_Avg (for future predictions)\n",
        "        row_for_prediction = data.iloc[-1, -15:].values\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_data = scaler.fit_transform(data[['Close'] + [f'Lag_{i}' for i in range(1, 11)]])\n",
        "        predicted_values = predict_next_7_days(model, scaler, row_for_prediction[1:])\n",
        "\n",
        "    # Now you can safely use predicted_values in the for loop:\n",
        "    print(\"\\nNext 7 Days Predictions:\")\n",
        "    for date, predicted in zip(future_dates, predicted_values):\n",
        "        print(f\"{date.date()}: Predicted Close = {predicted:.2f}\")\n",
        "\n",
        "    return scaled_data,scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLMECqGNwI1R",
        "outputId": "241c4210-b195-4712-c565-fa5fe33510f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing VEDL...\n",
            "Processing UPL...\n",
            "Processing ONGC...\n",
            "Processing ZEEL...\n",
            "Processing TCS...\n",
            "Processing ULTRACEMCO...\n",
            "Processing WIPRO...\n",
            "Processing TECHM...\n",
            "Processing TATASTEEL...\n",
            "Processing TITAN...\n",
            "             Date   Symbol Series  Prev Close     Open     High      Low  \\\n",
            "0      2011-06-01  SESAGOA     EQ      289.90   289.90   295.45   289.90   \n",
            "1      2011-06-02  SESAGOA     EQ      293.35   289.65   289.75   282.10   \n",
            "2      2011-06-03  SESAGOA     EQ      283.05   283.30   291.70   283.10   \n",
            "3      2011-06-06  SESAGOA     EQ      289.30   288.60   289.85   281.10   \n",
            "4      2011-06-07  SESAGOA     EQ      285.30   284.00   288.90   282.40   \n",
            "...           ...      ...    ...         ...      ...      ...      ...   \n",
            "24555  2021-04-26    TITAN     EQ     1461.45  1470.00  1489.75  1465.15   \n",
            "24556  2021-04-27    TITAN     EQ     1479.85  1485.00  1503.55  1482.00   \n",
            "24557  2021-04-28    TITAN     EQ     1495.10  1505.00  1514.90  1491.70   \n",
            "24558  2021-04-29    TITAN     EQ     1508.85  1524.00  1527.50  1497.00   \n",
            "24559  2021-04-30    TITAN     EQ     1506.80  1500.00  1516.55  1471.00   \n",
            "\n",
            "          Last    Close     VWAP   Volume      Turnover    Trades  \\\n",
            "0       293.50   293.35   293.28  1196748  3.509881e+13   20562.0   \n",
            "1       283.10   283.05   284.50  1370351  3.898713e+13   21567.0   \n",
            "2       289.20   289.30   288.12  1907629  5.496201e+13   30186.0   \n",
            "3       285.45   285.30   284.52   948796  2.699528e+13   16384.0   \n",
            "4       288.00   287.40   285.46  1268778  3.621888e+13   31608.0   \n",
            "...        ...      ...      ...      ...           ...       ...   \n",
            "24555  1478.95  1479.85  1479.58  1020499  1.509913e+14   39416.0   \n",
            "24556  1498.00  1495.10  1494.21  1145908  1.712228e+14   42363.0   \n",
            "24557  1509.00  1508.85  1504.73  1037771  1.561564e+14   37985.0   \n",
            "24558  1504.35  1506.80  1510.30  1927546  2.911181e+14   63568.0   \n",
            "24559  1489.85  1491.65  1493.26  3691399  5.512201e+14  111986.0   \n",
            "\n",
            "       Deliverable Volume  %Deliverble Ticker  Rolling_Avg  \n",
            "0                274456.0       0.2293   VEDL       284.96  \n",
            "1                480270.0       0.3505   VEDL       286.09  \n",
            "2                358179.0       0.1878   VEDL       287.62  \n",
            "3                135482.0       0.1428   VEDL       288.18  \n",
            "4                406505.0       0.3204   VEDL       287.68  \n",
            "...                   ...          ...    ...          ...  \n",
            "24555            228386.0       0.2238  TITAN      1492.75  \n",
            "24556            298862.0       0.2608  TITAN      1488.03  \n",
            "24557            212098.0       0.2044  TITAN      1485.22  \n",
            "24558            492144.0       0.2553  TITAN      1490.41  \n",
            "24559            699586.0       0.1895  TITAN      1496.45  \n",
            "\n",
            "[24560 rows x 17 columns]\n"
          ]
        }
      ],
      "source": [
        "tickers = [f.split('.')[0] for f in os.listdir('TRAIN') if f.endswith('.csv')]\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "all_data = []\n",
        "\n",
        "# Loop over each ticker and process\n",
        "for ticker_value in tickers:\n",
        "    print(f\"Processing {ticker_value}...\")\n",
        "\n",
        "    # Load the stock data for the current ticker\n",
        "    data = pd.read_csv(f'TRAIN/{ticker_value}.csv')\n",
        "\n",
        "    # Add a column to identify the ticker\n",
        "    data['Ticker'] = ticker_value\n",
        "\n",
        "    # Add the rolling average column\n",
        "    data['Rolling_Avg'] = data['Close'].rolling(window=window_size).mean()\n",
        "    data.dropna(inplace=True)  # Remove rows with NaN values caused by rolling\n",
        "    all_data.append(data)\n",
        "    combined_data = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Print the combined DataFrame with the rolling average\n",
        "print(combined_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "xbwzu5Rr0bR0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "6hsnJu6eBK54"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "uHyf0Z9M6wn8"
      },
      "outputs": [],
      "source": [
        " # Example window size\n",
        "data['Rolling_Avg'] = data['Close'].rolling(window=window_size).mean()\n",
        "data.dropna(inplace=True)  # Remove rows with NaN values caused by rolling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7JCCm2kY0gVo",
        "outputId": "50a0b60e-d93d-4719-ce8e-64409f62b57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date 2005/02/02 is not present in the dataset. Predicting future values...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.float64).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-390-e913e569ec47>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_for_user_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TCS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2005/02/02'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-388-0737fed3cb45>\u001b[0m in \u001b[0;36mpredict_for_user_input\u001b[0;34m(ticker_value, date_input)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'Lag_{i}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpredicted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_next_7_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_for_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Now you can safely use predicted_values in the for loop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-381-5c8952ecf350>\u001b[0m in \u001b[0;36mpredict_next_7_days\u001b[0;34m(model, scaler, last_known_data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_known_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnext_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.float64)."
          ]
        }
      ],
      "source": [
        "scaled_data,scaler=predict_for_user_input('TCS','2005/02/02')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "yLs_an7S0lUG"
      },
      "outputs": [],
      "source": [
        "X = scaled_data[:, 1:]  # All columns except the target (Close)\n",
        "y = scaled_data[:, 0]   # Target is the 'Close' column\n",
        "\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Reshape for LSTM input\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "MOOFxnn21CZ7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN2xLl2-0oMu",
        "outputId": "e1d87ec0-cf05-43d6-a7b5-f98596d5b4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.0047 - val_loss: 0.0052\n",
            "Epoch 2/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 3/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.0013 - val_loss: 6.0896e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 5.0812e-04 - val_loss: 9.7014e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 3.9565e-04 - val_loss: 1.6653e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.9495e-04 - val_loss: 6.3768e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 4.8897e-04 - val_loss: 1.9423e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.2457e-04 - val_loss: 9.7215e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2824e-04 - val_loss: 1.9143e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.2701e-04 - val_loss: 3.4091e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 5.0871e-04 - val_loss: 0.0015\n",
            "Epoch 12/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3556e-04 - val_loss: 1.0401e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.2249e-04 - val_loss: 2.2878e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.1107e-05 - val_loss: 3.1400e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6712e-04 - val_loss: 3.7482e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1958e-04 - val_loss: 1.5787e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.2776e-04 - val_loss: 1.1365e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 4.5033e-04 - val_loss: 2.3376e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.4342e-04 - val_loss: 5.0289e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.8884e-04 - val_loss: 8.2628e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x785b28591570>"
            ]
          },
          "execution_count": 386,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=15, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "UaUNudbF1GuI",
        "outputId": "a8610029-9a35-4e36-ca76-e01fe9536088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step\n"
          ]
        },
        {
          "ename": "NotFittedError",
          "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-387-25f499ac2e45>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# For actuals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mTransformed\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Placeholder for predictions inverse transformation\n",
        "predictions_full = np.zeros((predictions.shape[0], scaled_data.shape[1]))\n",
        "predictions_full[:, 0] = predictions.flatten()\n",
        "predictions_inverse = scaler.inverse_transform(predictions_full)[:, 0]\n",
        "\n",
        "# For actuals\n",
        "actuals_full = np.zeros((y_test.shape[0], scaled_data.shape[1]))\n",
        "actuals_full[:, 0] = y_test.flatten()\n",
        "actuals_inverse = scaler.inverse_transform(actuals_full)[:, 0]\n",
        "\n",
        "\n",
        "# Include Rolling Average for comparison\n",
        "rolling_avg_actuals = scaler.inverse_transform(actuals_full)[:, 1]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actuals_inverse, label='Actual Close Values')\n",
        "plt.plot(rolling_avg_actuals, label='Rolling Average')\n",
        "plt.plot(predictions_inverse, label='Predicted Close Values')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Rolling Average vs Predicted ('+ticker_value+')')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Close Value')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjYOoE3BafdN"
      },
      "outputs": [],
      "source": [
        "data.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EM30hE5IHO9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-VkZSL21YAN"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "#last_date = pd.to_datetime(data['Date'].iloc[-1])  # Convert last_date to datetime\n",
        "future_dates = [last_date + pd.Timedelta(days=i) for i in range(1, 8)]\n",
        "print(\"\\nNext 7 Days Predictions:\")\n",
        "for date, value in zip(future_dates, predicted_values):\n",
        "    print(f\"{date.date()}: Predicted Close = {value:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
